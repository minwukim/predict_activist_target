{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efbe685-7894-4519-a3f7-99a1903a2403",
   "metadata": {},
   "source": [
    "# 3. Data Imputation\n",
    "\n",
    "In this notebook, we conduct data imputation with these methods as listed below:\n",
    "- Median imputation\n",
    "- KNN (K=5)\n",
    "- MiceForest\n",
    "- GAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd14277-e9ca-405c-8a7a-0c3ba288e058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/minwukim/anaconda3/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "import miceforest as mf\n",
    "from tqdm import tqdm\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00e0d890-9b5d-4284-8e3f-a36fab3c0ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kw/6hnl881s2p9701b42gpjt_pr0000gn/T/ipykernel_20460/2360894533.py:1: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../database/companies/original/2016-2023.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../database/companies/original/2016-2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e85cea9-4739-4f05-831d-4779e88c8135",
   "metadata": {},
   "outputs": [],
   "source": [
    "governance = [\"ceo_is_female\",\"unequal_voting\",\"ceo_tenure\",\"board_size\",\"classified_board_system\",\"poison_pill\",\"buyback_yield\",\n",
    "              \"dividend_payout_ratio\",\"cf_to_total_compensation_to_executives\",\"cf_to_total_compensation_to_board_members\"]\n",
    "\n",
    "operation = [\"cf_to_capex_industry_peers_percentile\",\"net_debt_to_ebitda_industry_peers_percentile\",\n",
    "             \"current_ratio_industry_peers_percentile\",\"ebitda_margin_industry_peers_percentile\",\n",
    "             \"sales_to_total_assets_industry_peers_percentile\",\"employee_growth_rate_industry_peers_percentile\",\n",
    "             \"fcf_yield_industry_peers_percentile\",\"sales_growth_rate_industry_peers_percentile\",\n",
    "             \"cash_conversion_cycle_industry_peers_percentile\",\"interest_coverage_ratio_industry_peers_percentile\"]\n",
    "\n",
    "ownership = [\"free_float_percentage\",\"institution_ownership_percentage\",\"insider_shares_percentage\"]\n",
    "\n",
    "technical = [\"rsi_14_30_average\",\"volatility_30_90_180_average\",\"volume_30d_average_to_outstanding\"]\n",
    "\n",
    "returns = [\"total_return_5y_4y_3y_2y_average\",\"total_return_1y_6m_3m_average\"]\n",
    "\n",
    "valuation = [\"roe_industry_peers_percentile\",\"operating_roic_industry_peers_percentile\",\"pe_ratio_industry_peers_percentile\",\n",
    "             \"eps_industry_peers_percentile\",\"ev_to_sales_industry_peers_percentile\",\"tobin_q_ratio_industry_peers_percentile\",\n",
    "             \"pb_ratio_industry_peers_percentile\",\"asset_to_equity_industry_peers_percentile\",\"ev_ebitda_industry_peers_percentile\",\"ev_to_asset_industry_peers_percentile\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17f30edd-2104-4fb8-9e01-8197f07d0f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_original = [\"cf_to_capex\",\"net_debt_to_ebitda\",\"current_ratio\",\"ebitda_margin\",\"sales_to_total_assets\",\n",
    "                      \"employee_growth_rate\",\"fcf_yield\",\"sales_growth_rate\",\"cash_conversion_cycle\",\"interest_coverage_ratio\"]\n",
    "\n",
    "technical_original = ['rsi_14d','rsi_30d','volatility_30d','volatility_90d','volatility_180d',\"volume_30d_average_to_outstanding\"]\n",
    "\n",
    "returns_original = ['total_return_5y', 'total_return_4y', 'total_return_3y','total_return_2y', 'total_return_1y', 'total_return_6m','total_return_3m']\n",
    "\n",
    "valuation_original = [\"roe\",\"operating_roic\",\"pe_ratio\",\"eps\",\"ev_to_sales\",\"tobin_q_ratio\",\"pb_ratio\",\"asset_to_equity\",\"ev_ebitda\",\"ev_to_asset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789352bd-da6c-425c-b532-ef775e302059",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bic_level_2\"] = df[\"bic_level_2\"].astype('category')\n",
    "df[\"bic_level_3\"] = df[\"bic_level_3\"].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7145f-3da6-40ff-a050-0fb670faad27",
   "metadata": {},
   "source": [
    "Each group, from group_1 to group_6, undergoes imputation collectively along with the one-hot encoded year column. \n",
    "In contrast, every column within the median_group is subjected to median imputation individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2afb5e6b-040c-4fdc-8923-a955d74a2166",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_group = ['ceo_is_female','unequal_voting', 'ceo_tenure', 'board_size', 'classified_board_system', 'poison_pill']\n",
    "group_1 = ['buyback_yield', 'dividend_payout_ratio', 'cf_to_total_compensation_to_executives', 'cf_to_total_compensation_to_board_members']\n",
    "group_2 = operation_original\n",
    "group_3 = ownership\n",
    "group_4 = technical_original\n",
    "group_5 = returns_original\n",
    "group_6 = valuation_original\n",
    "year_one_hot_encoding = ['year_2016','year_2017','year_2018','year_2019','year_2020','year_2021','year_2022']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a833b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percentile_combined(df, col):\n",
    "    def compute_percentile(group):\n",
    "        # Return percentile rank or NaN for small groups\n",
    "        return group.rank(pct=True) * 100 if len(group) >= 10 else pd.Series(np.nan, index=group.index)\n",
    "\n",
    "    # Compute percentile based on bic_level_3\n",
    "    level_3_percentile = df.groupby(['year', 'bic_level_3'], observed=False)[col].transform(compute_percentile)\n",
    "    \n",
    "    # Where BIC Level 3 groups are smaller than 10, fallback to BIC Level 2\n",
    "    mask_fallback = level_3_percentile.isna()\n",
    "    level_2_percentile = df.groupby(['year', 'bic_level_2'], observed=False)[col].transform(compute_percentile)\n",
    "    level_3_percentile[mask_fallback] = level_2_percentile[mask_fallback]\n",
    "\n",
    "    return level_3_percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73badef0-eddd-49e3-a31e-15024b9fd11e",
   "metadata": {},
   "source": [
    "## A. Median Imputation\n",
    "- For features end with industry_peer_percentile_values, we impute 50. \n",
    "- For the rest, we group by the year and impute the median value. Same for the binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87466c5-29ac-4a16-9b5d-3887745eff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_median = df.copy()\n",
    "\n",
    "median_columns = governance + ownership + technical_original + returns_original\n",
    "constant_50_columns = operation + valuation\n",
    "\n",
    "# Impute median for selected columns grouped by 'year' in the copy\n",
    "df_median[median_columns] = df_median.groupby('year')[median_columns].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Impute 50 for selected columns in the copy\n",
    "df_median[constant_50_columns] = df_median[constant_50_columns].fillna(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a542436-9845-47b4-9314-c80ccf908f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_median.to_csv('../database/companies/imputation/median/median_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a8f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['year']!=2023]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9afdf-6ab8-4f97-a70d-5236c757d1cf",
   "metadata": {},
   "source": [
    "## B. KNN Imputation (K = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4091c263-c303-4c0c-b79b-ca6829e5e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn = df.copy()\n",
    "\n",
    "# Function to perform KNN Imputation\n",
    "def knn_impute(df, group_cols, year_cols):\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    imputed_data = imputer.fit_transform(df[group_cols + year_cols])\n",
    "    return pd.DataFrame(imputed_data, columns=group_cols + year_cols, index=df.index)\n",
    "\n",
    "# Perform KNN Imputation for each group\n",
    "df_knn[group_1 + year_one_hot_encoding] = knn_impute(df_knn, group_1, year_one_hot_encoding)\n",
    "df_knn[group_2 + year_one_hot_encoding] = knn_impute(df_knn, group_2, year_one_hot_encoding)\n",
    "df_knn[group_3 + year_one_hot_encoding] = knn_impute(df_knn, group_3, year_one_hot_encoding)\n",
    "df_knn[group_4 + year_one_hot_encoding] = knn_impute(df_knn, group_4, year_one_hot_encoding)\n",
    "df_knn[group_5 + year_one_hot_encoding] = knn_impute(df_knn, group_5, year_one_hot_encoding)\n",
    "df_knn[group_6 + year_one_hot_encoding] = knn_impute(df_knn, group_6, year_one_hot_encoding)\n",
    "\n",
    "# Median Imputation for median_group\n",
    "df_knn[median_group] = df_median.groupby('year')[median_group].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2d3dc98-3280-48cc-a99f-187d326b64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn['rsi_14_30_average'] = df_knn[['rsi_14d','rsi_30d']].mean(axis=1, skipna=True)\n",
    "df_knn['volatility_30_90_180_average'] = df_knn[['volatility_30d','volatility_90d','volatility_180d']].mean(axis=1, skipna=True)\n",
    "\n",
    "df_knn['total_return_5y_4y_3y_2y_average'] = df_knn[['total_return_5y', 'total_return_4y', 'total_return_3y', 'total_return_2y']].mean(axis=1, skipna=True)\n",
    "df_knn['total_return_1y_6m_3m_average'] = df_knn[['total_return_1y', 'total_return_6m', 'total_return_3m']].mean(axis=1, skipna=True)\n",
    "\n",
    "for col in operation_original + valuation_original:\n",
    "    percentile_col = col + '_industry_peers_percentile'\n",
    "    df_knn[percentile_col] = compute_percentile_combined(df_knn, col)\n",
    "    df_knn[percentile_col] = df_knn[percentile_col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e25e32db-fc88-4397-b3db-1e1f13476807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn.to_csv('../database/companies/imputation/kNN/kNN_original.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939aa69-4461-4d6b-9b5a-6224302f3e8b",
   "metadata": {},
   "source": [
    "## C. MiceForest Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c52e639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original dataframe\n",
    "df_miceforest = df.copy()\n",
    "\n",
    "# List of groups\n",
    "groups = [group_1, group_2, group_3, group_4, group_5, group_6]\n",
    "\n",
    "# Perform imputation for each group\n",
    "for group in groups:\n",
    "    # Creating a kernel only with the columns in the current group\n",
    "    kernel = mf.ImputationKernel(df_miceforest[group+year_one_hot_encoding], save_all_iterations=True, random_state=1991)\n",
    "\n",
    "    # Running the MICE algorithm\n",
    "    kernel.mice(3)  # Number of iterations\n",
    "\n",
    "    # Updating the columns in the original dataset\n",
    "    df_miceforest[group] = kernel.complete_data()[group]\n",
    "\n",
    "# Median Imputation for median_group\n",
    "df_miceforest[median_group] = df_miceforest.groupby('year')[median_group].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6133cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miceforest['rsi_14_30_average'] = df_miceforest[['rsi_14d','rsi_30d']].mean(axis=1, skipna=True)\n",
    "df_miceforest['volatility_30_90_180_average'] = df_miceforest[['volatility_30d','volatility_90d','volatility_180d']].mean(axis=1, skipna=True)\n",
    "\n",
    "df_miceforest['total_return_5y_4y_3y_2y_average'] = df_miceforest[['total_return_5y', 'total_return_4y', 'total_return_3y', 'total_return_2y']].mean(axis=1, skipna=True)\n",
    "df_miceforest['total_return_1y_6m_3m_average'] = df_miceforest[['total_return_1y', 'total_return_6m', 'total_return_3m']].mean(axis=1, skipna=True)\n",
    "\n",
    "for col in operation_original + valuation_original:\n",
    "    percentile_col = col + '_industry_peers_percentile'\n",
    "    df_miceforest[percentile_col] = compute_percentile_combined(df_miceforest, col)\n",
    "    df_miceforest[percentile_col] = df_miceforest[percentile_col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b24cacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miceforest.to_csv('../database/companies/imputation/MiceForest/MiceForest_original.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc81e509",
   "metadata": {},
   "source": [
    "## D. GAIN Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbffbd8c",
   "metadata": {},
   "source": [
    "[Reference: Imputation with GAIN](https://www.kaggle.com/code/youseefmoemen/generative-adversarial-imputation-network-gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8778f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization (data, parameters=None):\n",
    "  '''Normalize data in [0, 1] range.\n",
    "  \n",
    "  Args:\n",
    "    - data: original data\n",
    "  \n",
    "  Returns:\n",
    "    - norm_data: normalized data\n",
    "    - norm_parameters: min_val, max_val for each feature for renormalization\n",
    "  '''\n",
    "\n",
    "  # Parameters\n",
    "  _, dim = data.shape\n",
    "  norm_data = data.copy()\n",
    "  \n",
    "  if parameters is None:\n",
    "  \n",
    "    # MixMax normalization\n",
    "    min_val = np.zeros(dim)\n",
    "    max_val = np.zeros(dim)\n",
    "    \n",
    "    # For each dimension\n",
    "    for i in range(dim):\n",
    "      min_val[i] = np.nanmin(norm_data[:,i])\n",
    "      norm_data[:,i] = norm_data[:,i] - np.nanmin(norm_data[:,i])\n",
    "      max_val[i] = np.nanmax(norm_data[:,i])\n",
    "      norm_data[:,i] = norm_data[:,i] / (np.nanmax(norm_data[:,i]) + 1e-6)   \n",
    "      \n",
    "    # Return norm_parameters for renormalization\n",
    "    norm_parameters = {'min_val': min_val,\n",
    "                       'max_val': max_val}\n",
    "\n",
    "  else:\n",
    "    min_val = parameters['min_val']\n",
    "    max_val = parameters['max_val']\n",
    "    \n",
    "    # For each dimension\n",
    "    for i in range(dim):\n",
    "      norm_data[:,i] = norm_data[:,i] - min_val[i]\n",
    "      norm_data[:,i] = norm_data[:,i] / (max_val[i] + 1e-6)  \n",
    "      \n",
    "    norm_parameters = parameters    \n",
    "      \n",
    "  return norm_data, norm_parameters\n",
    "\n",
    "\n",
    "def renormalization (norm_data, norm_parameters):\n",
    "  '''Renormalize data from [0, 1] range to the original range.\n",
    "  \n",
    "  Args:\n",
    "    - norm_data: normalized data\n",
    "    - norm_parameters: min_val, max_val for each feature for renormalization\n",
    "  \n",
    "  Returns:\n",
    "    - renorm_data: renormalized original data\n",
    "  '''\n",
    "  \n",
    "  min_val = norm_parameters['min_val']\n",
    "  max_val = norm_parameters['max_val']\n",
    "\n",
    "  _, dim = norm_data.shape\n",
    "  renorm_data = norm_data.copy()\n",
    "    \n",
    "  for i in range(dim):\n",
    "    renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)   \n",
    "    renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n",
    "    \n",
    "  return renorm_data\n",
    "\n",
    "\n",
    "def rounding (imputed_data, data_x):\n",
    "  '''Round imputed data for categorical variables.\n",
    "  \n",
    "  Args:\n",
    "    - imputed_data: imputed data\n",
    "    - data_x: original data with missing values\n",
    "    \n",
    "  Returns:\n",
    "    - rounded_data: rounded imputed data\n",
    "  '''\n",
    "  \n",
    "  _, dim = data_x.shape\n",
    "  rounded_data = imputed_data.copy()\n",
    "  \n",
    "  for i in range(dim):\n",
    "    temp = data_x[~np.isnan(data_x[:, i]), i]\n",
    "    # Only for the categorical variable\n",
    "    if len(np.unique(temp)) < 20:\n",
    "      rounded_data[:, i] = np.round(rounded_data[:, i])\n",
    "      \n",
    "  return rounded_data\n",
    "\n",
    "\n",
    "def rmse_loss (ori_data, imputed_data, data_m):\n",
    "  '''Compute RMSE loss between ori_data and imputed_data\n",
    "  \n",
    "  Args:\n",
    "    - ori_data: original data without missing values\n",
    "    - imputed_data: imputed data\n",
    "    - data_m: indicator matrix for missingness\n",
    "    \n",
    "  Returns:\n",
    "    - rmse: Root Mean Squared Error\n",
    "  '''\n",
    "  \n",
    "  ori_data, norm_parameters = normalization(ori_data)\n",
    "  imputed_data, _ = normalization(imputed_data, norm_parameters)\n",
    "    \n",
    "  # Only for missing values\n",
    "  nominator = np.sum(((1-data_m) * ori_data - (1-data_m) * imputed_data)**2)\n",
    "  denominator = np.sum(1-data_m)\n",
    "  \n",
    "  rmse = np.sqrt(nominator/float(denominator))\n",
    "  \n",
    "  return rmse\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "  '''Xavier initialization.\n",
    "  \n",
    "  Args:\n",
    "    - size: vector size\n",
    "    \n",
    "  Returns:\n",
    "    - initialized random vector.\n",
    "  '''\n",
    "  in_dim = size[0]\n",
    "  xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "  return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "      \n",
    "\n",
    "def binary_sampler(p, rows, cols):\n",
    "  '''Sample binary random variables.\n",
    "  \n",
    "  Args:\n",
    "    - p: probability of 1\n",
    "    - rows: the number of rows\n",
    "    - cols: the number of columns\n",
    "    \n",
    "  Returns:\n",
    "    - binary_random_matrix: generated binary random matrix.\n",
    "  '''\n",
    "  unif_random_matrix = np.random.uniform(0., 1., size = [rows, cols])\n",
    "  binary_random_matrix = 1*(unif_random_matrix < p)\n",
    "  return binary_random_matrix\n",
    "\n",
    "\n",
    "def uniform_sampler(low, high, rows, cols):\n",
    "  '''Sample uniform random variables.\n",
    "  \n",
    "  Args:\n",
    "    - low: low limit\n",
    "    - high: high limit\n",
    "    - rows: the number of rows\n",
    "    - cols: the number of columns\n",
    "    \n",
    "  Returns:\n",
    "    - uniform_random_matrix: generated uniform random matrix.\n",
    "  '''\n",
    "  return np.random.uniform(low, high, size = [rows, cols])       \n",
    "\n",
    "\n",
    "def sample_batch_index(total, batch_size):\n",
    "  '''Sample index of the mini-batch.\n",
    "  \n",
    "  Args:\n",
    "    - total: total number of samples\n",
    "    - batch_size: batch size\n",
    "    \n",
    "  Returns:\n",
    "    - batch_idx: batch index\n",
    "  '''\n",
    "  total_idx = np.random.permutation(total)\n",
    "  batch_idx = total_idx[:batch_size]\n",
    "  return batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbbb0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain (data_x, gain_parameters):\n",
    "  '''Impute missing values in data_x\n",
    "  \n",
    "  Args:\n",
    "    - data_x: original data with missing values\n",
    "    - gain_parameters: GAIN network parameters:\n",
    "      - batch_size: Batch size\n",
    "      - hint_rate: Hint rate\n",
    "      - alpha: Hyperparameter\n",
    "      - iterations: Iterations\n",
    "      \n",
    "  Returns:\n",
    "    - imputed_data: imputed data\n",
    "  '''\n",
    "  # Define mask matrix\n",
    "  data_m = 1-np.isnan(data_x)\n",
    "  \n",
    "  # System parameters\n",
    "  batch_size = gain_parameters['batch_size']\n",
    "  hint_rate = gain_parameters['hint_rate']\n",
    "  alpha = gain_parameters['alpha']\n",
    "  iterations = gain_parameters['iterations']\n",
    "  \n",
    "  # Other parameters\n",
    "  no, dim = data_x.shape\n",
    "  \n",
    "  # Hidden state dimensions\n",
    "  h_dim = int(dim)\n",
    "  \n",
    "  # Normalization\n",
    "  norm_data, norm_parameters = normalization(data_x)\n",
    "  norm_data_x = np.nan_to_num(norm_data, 0)\n",
    "  \n",
    "  ## GAIN architecture   \n",
    "  # Input placeholders\n",
    "  # Data vector\n",
    "  X = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  # Mask vector \n",
    "  M = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  # Hint vector\n",
    "  H = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  \n",
    "  # Discriminator variables\n",
    "  D_W1 = tf.Variable(xavier_init([dim*2, h_dim])) # Data + Hint as inputs\n",
    "  D_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  D_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
    "  D_b2 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  D_W3 = tf.Variable(xavier_init([h_dim, dim]))\n",
    "  D_b3 = tf.Variable(tf.zeros(shape = [dim]))  # Multi-variate outputs\n",
    "  \n",
    "  theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "  \n",
    "  #Generator variables\n",
    "  # Data + Mask as inputs (Random noise is in missing components)\n",
    "  G_W1 = tf.Variable(xavier_init([dim*2, h_dim]))  \n",
    "  G_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  G_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
    "  G_b2 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  G_W3 = tf.Variable(xavier_init([h_dim, dim]))\n",
    "  G_b3 = tf.Variable(tf.zeros(shape = [dim]))\n",
    "  \n",
    "  theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
    "  \n",
    "  ## GAIN functions\n",
    "  # Generator\n",
    "  def generator(x,m):\n",
    "    # Concatenate Mask and Data\n",
    "    inputs = tf.concat(values = [x, m], axis = 1) \n",
    "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)   \n",
    "    # MinMax normalized output\n",
    "    G_prob = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3) \n",
    "    return G_prob\n",
    "      \n",
    "  # Discriminator\n",
    "  def discriminator(x, h):\n",
    "    # Concatenate Data and Hint\n",
    "    inputs = tf.concat(values = [x, h], axis = 1) \n",
    "    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n",
    "    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return D_prob\n",
    "  \n",
    "  ## GAIN structure\n",
    "  # Generator\n",
    "  G_sample = generator(X, M)\n",
    " \n",
    "  # Combine with observed data\n",
    "  Hat_X = X * M + G_sample * (1-M)\n",
    "  \n",
    "  # Discriminator\n",
    "  D_prob = discriminator(Hat_X, H)\n",
    "  \n",
    "  ## GAIN loss\n",
    "  D_loss_temp = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) \\\n",
    "                                + (1-M) * tf.log(1. - D_prob + 1e-8)) \n",
    "  \n",
    "  G_loss_temp = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\n",
    "  \n",
    "  MSE_loss = \\\n",
    "  tf.reduce_mean((M * X - M * G_sample)**2) / tf.reduce_mean(M)\n",
    "  \n",
    "  D_loss = D_loss_temp\n",
    "  G_loss = G_loss_temp + alpha * MSE_loss \n",
    "  \n",
    "  ## GAIN solver\n",
    "  D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "  G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "  \n",
    "  ## Iterations\n",
    "  sess = tf.Session()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "  # Start Iterations\n",
    "  for it in tqdm(range(iterations)):    \n",
    "      \n",
    "    # Sample batch\n",
    "    batch_idx = sample_batch_index(no, batch_size)\n",
    "    X_mb = norm_data_x[batch_idx, :]  \n",
    "    M_mb = data_m[batch_idx, :]  \n",
    "    # Sample random vectors  \n",
    "    Z_mb = uniform_sampler(0, 0.01, batch_size, dim) \n",
    "    # Sample hint vectors\n",
    "    H_mb_temp = binary_sampler(hint_rate, batch_size, dim)\n",
    "    H_mb = M_mb * H_mb_temp\n",
    "      \n",
    "    # Combine random vectors with observed vectors\n",
    "    X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
    "      \n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss_temp], \n",
    "                              feed_dict = {M: M_mb, X: X_mb, H: H_mb})\n",
    "    _, G_loss_curr, MSE_loss_curr = \\\n",
    "    sess.run([G_solver, G_loss_temp, MSE_loss],\n",
    "             feed_dict = {X: X_mb, M: M_mb, H: H_mb})\n",
    "            \n",
    "  ## Return imputed data      \n",
    "  Z_mb = uniform_sampler(0, 0.01, no, dim) \n",
    "  M_mb = data_m\n",
    "  X_mb = norm_data_x          \n",
    "  X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
    "      \n",
    "  imputed_data = sess.run([G_sample], feed_dict = {X: X_mb, M: M_mb})[0]\n",
    "  \n",
    "  imputed_data = data_m * norm_data_x + (1-data_m) * imputed_data\n",
    "  \n",
    "  # Renormalization\n",
    "  imputed_data = renormalization(imputed_data, norm_parameters)  \n",
    "  \n",
    "  # Rounding\n",
    "  imputed_data = rounding(imputed_data, data_x)  \n",
    "          \n",
    "  return imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1c764c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 17:32:59.016411: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1618.25it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1537.31it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1641.68it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1596.05it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1460.05it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1485.88it/s]\n"
     ]
    }
   ],
   "source": [
    "df_GAIN = df.copy()\n",
    "\n",
    "gain_parameters = {\n",
    "    'batch_size': 128,\n",
    "    'hint_rate': 0.9,\n",
    "    'alpha': 10,\n",
    "    'iterations': 1000\n",
    "}\n",
    "\n",
    "groups = [group_1, group_2, group_3, group_4, group_5, group_6]\n",
    "\n",
    "for group in groups:\n",
    "    data_x = df_GAIN[group].values\n",
    "    imputed_data = gain(data_x, gain_parameters)\n",
    "    df_GAIN[group] = np.where(np.isnan(df_GAIN[group]), imputed_data, df_GAIN[group])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5383b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GAIN[median_group] = df_GAIN.groupby('year')[median_group].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "df_GAIN['rsi_14_30_average'] = df_GAIN[['rsi_14d','rsi_30d']].mean(axis=1, skipna=True)\n",
    "df_GAIN['volatility_30_90_180_average'] = df_GAIN[['volatility_30d','volatility_90d','volatility_180d']].mean(axis=1, skipna=True)\n",
    "\n",
    "df_GAIN['total_return_5y_4y_3y_2y_average'] = df_GAIN[['total_return_5y', 'total_return_4y', 'total_return_3y', 'total_return_2y']].mean(axis=1, skipna=True)\n",
    "df_GAIN['total_return_1y_6m_3m_average'] = df_GAIN[['total_return_1y', 'total_return_6m', 'total_return_3m']].mean(axis=1, skipna=True)\n",
    "\n",
    "for col in operation_original + valuation_original:\n",
    "    percentile_col = col + '_industry_peers_percentile'\n",
    "    df_GAIN[percentile_col] = compute_percentile_combined(df_miceforest, col)\n",
    "    df_GAIN[percentile_col] = df_GAIN[percentile_col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bae2df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GAIN.to_csv('../database/companies/imputation/GAIN/GAIN_original.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
